{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Learning Hybrid Chatbot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import openai # type: ignore\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "from tensorflow.keras import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout # type: ignore\n",
    "from japanese_dataset import japanese_dataset, explanations\n",
    "from openai import OpenAI # type: ignore\n",
    "client = OpenAI(api_key='')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and installing NLTK\n",
    "NLTK(Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "[Natural Language Processing with Python](http://www.nltk.org/book/) provides a practical introduction to programming for language processing.\n",
    "\n",
    "For platform-specific instructions, read [here](https://www.nltk.org/install.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\Muhammad Daffa A B\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # type: ignore\n",
    "from nltk.stem import WordNetLemmatizer # type: ignore\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "For our example, we'll be utilizing a JSON file called intents_exercise as a dataset that contains multiple intents with its corresponding patterns and suitable replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'tag': 'katakanaDefinition', 'patterns': ['Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'tag': 'hiraganaDefinition', 'patterns': ['Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'tag': 'katakanaExamples', 'patterns': ['Show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'tag': 'hiraganaExamples', 'patterns': ['Show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'tag': 'katakanaUsage', 'patterns': ['When to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'tag': 'katakanaComprehensionChallenges', 'pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'tag': 'katakanaAndLanguageDevelopment', 'pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>{'tag': 'katakanaProcessingEfficiency', 'patte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>{'tag': 'katakanaAndMentalMapping', 'patterns'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>{'tag': 'thanks', 'patterns': ['Thank you', 'T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              intents\n",
       "0   {'tag': 'katakanaDefinition', 'patterns': ['Wh...\n",
       "1   {'tag': 'hiraganaDefinition', 'patterns': ['Wh...\n",
       "2   {'tag': 'katakanaExamples', 'patterns': ['Show...\n",
       "3   {'tag': 'hiraganaExamples', 'patterns': ['Show...\n",
       "4   {'tag': 'katakanaUsage', 'patterns': ['When to...\n",
       "..                                                ...\n",
       "58  {'tag': 'katakanaComprehensionChallenges', 'pa...\n",
       "59  {'tag': 'katakanaAndLanguageDevelopment', 'pat...\n",
       "60  {'tag': 'katakanaProcessingEfficiency', 'patte...\n",
       "61  {'tag': 'katakanaAndMentalMapping', 'patterns'...\n",
       "62  {'tag': 'thanks', 'patterns': ['Thank you', 'T...\n",
       "\n",
       "[63 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"./intents_exercise.json\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing\n",
    "\n",
    "During the pre-processing step, it will iterate through each intent in the dataset and tokenize each pattern in the intents. The patterns (sentences) in the dataset need to be broken down into individual words or tokens. This is done using NLTK's word_tokenize() function, which splits the sentences into words.\n",
    "\n",
    "After that, lemmatization reduces words to their base or root form. For example, \"running\" becomes \"run\". This is important for reducing the vocabulary size and ensuring that similar words are treated the same way by the model. NLTK's WordNetLemmatizer is used for lemmatization.\n",
    "\n",
    "Punctuation marks don't carry significant meaning in the context of natural language understanding, so they are often removed. Additionally, converting all words to lowercase ensures that the model treats words like \"hello\" and \"Hello\" as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'and',\n",
       " 'are',\n",
       " 'art',\n",
       " 'between',\n",
       " 'book',\n",
       " 'calligraphy',\n",
       " 'challenge',\n",
       " 'character',\n",
       " 'child',\n",
       " 'cognitive',\n",
       " 'combination',\n",
       " 'compared',\n",
       " 'comprehension',\n",
       " 'conjugation',\n",
       " 'contribute',\n",
       " 'cultural',\n",
       " 'culture',\n",
       " 'custom',\n",
       " 'development',\n",
       " 'difference',\n",
       " 'difficulty',\n",
       " 'do',\n",
       " 'doe',\n",
       " 'education',\n",
       " 'efficiency',\n",
       " 'efficiently',\n",
       " 'emphasis',\n",
       " 'emphasized',\n",
       " 'emphasizing',\n",
       " 'enhance',\n",
       " 'evolution',\n",
       " 'example',\n",
       " 'face',\n",
       " 'fast',\n",
       " 'for',\n",
       " 'foreign',\n",
       " 'game',\n",
       " 'genre',\n",
       " 'hiragana',\n",
       " 'hiragana-only',\n",
       " 'historical',\n",
       " 'how',\n",
       " 'impact',\n",
       " 'in',\n",
       " 'influence',\n",
       " 'is',\n",
       " 'japanese',\n",
       " 'kanji',\n",
       " 'katakana',\n",
       " 'language',\n",
       " 'list',\n",
       " 'literary',\n",
       " 'literature',\n",
       " 'load',\n",
       " 'loanword',\n",
       " 'map',\n",
       " 'mapping',\n",
       " 'me',\n",
       " 'meaning',\n",
       " 'medium',\n",
       " 'mental',\n",
       " 'mentally',\n",
       " 'modern',\n",
       " 'much',\n",
       " 'name',\n",
       " 'native',\n",
       " 'no',\n",
       " 'of',\n",
       " 'on',\n",
       " 'onomatopoeia',\n",
       " 'origin',\n",
       " 'other',\n",
       " 'particle',\n",
       " 'perceive',\n",
       " 'perception',\n",
       " 'poetry',\n",
       " 'popular',\n",
       " 'practice',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'role',\n",
       " 'script',\n",
       " 'show',\n",
       " 'significance',\n",
       " 'speaker',\n",
       " 'speed',\n",
       " 'strategy',\n",
       " 'style',\n",
       " 'teaching',\n",
       " 'technical',\n",
       " 'technology',\n",
       " 'term',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'the',\n",
       " 'to',\n",
       " 'traditional',\n",
       " 'usage',\n",
       " 'use',\n",
       " 'used',\n",
       " 'v',\n",
       " 'various',\n",
       " 'verb',\n",
       " 'very',\n",
       " 'view',\n",
       " 'vs.',\n",
       " 'what',\n",
       " 'when',\n",
       " 'with',\n",
       " 'word',\n",
       " 'writing',\n",
       " 'you']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "data_X = []\n",
    "data_Y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        data_X.append(pattern)\n",
    "        data_Y.append(intent[\"tag\"]) ,\n",
    "\n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "for idx, doc in enumerate(data_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "        output_row = list(out_empty)\n",
    "        output_row[classes.index(data_Y[idx])] = 1\n",
    "        training.append([bow, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_Y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               14848     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 63)                4095      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27199 (106.25 KB)\n",
      "Trainable params: 27199 (106.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "345/345 [==============================] - 2s 3ms/step - loss: 1.1307 - accuracy: 0.6774 - val_loss: 0.0358 - val_accuracy: 0.9815\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.4538 - accuracy: 0.8495 - val_loss: 0.0159 - val_accuracy: 0.9920\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 1s 3ms/step - loss: 0.3935 - accuracy: 0.8746 - val_loss: 0.0143 - val_accuracy: 0.9920\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.3938 - accuracy: 0.8805 - val_loss: 0.0128 - val_accuracy: 0.9920\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.3885 - accuracy: 0.8862 - val_loss: 0.0117 - val_accuracy: 0.9920\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.4105 - accuracy: 0.8804 - val_loss: 0.0140 - val_accuracy: 0.9920\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.4153 - accuracy: 0.8832 - val_loss: 0.0119 - val_accuracy: 0.9920\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.4024 - accuracy: 0.8912 - val_loss: 0.0119 - val_accuracy: 0.9920\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.3874 - accuracy: 0.8978 - val_loss: 0.0135 - val_accuracy: 0.9920\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.3873 - accuracy: 0.9000 - val_loss: 0.0130 - val_accuracy: 0.9920\n",
      "Training Accuracy: 0.991576075553894\n",
      "Testing Accuracy: 0.9920290112495422\n",
      "The model's performance is consistent between training and testing data.\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_Y[0]), activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(x=train_X, y=train_Y, validation_data=(test_X, test_Y), epochs=10, verbose=1)\n",
    "# Evaluate on training data\n",
    "train_loss, train_accuracy = model.evaluate(train_X, train_Y, verbose=0)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Evaluate on testing data\n",
    "test_loss, test_accuracy = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# Compare accuracies\n",
    "if test_accuracy < train_accuracy:\n",
    "    print(\"The model might be overfitting.\")\n",
    "else:\n",
    "    print(\"The model's performance is consistent between training and testing data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_question():\n",
    "    \"\"\"Select a random question from the Japanese dataset\"\"\"\n",
    "    return random.choice(list(japanese_dataset.keys()))\n",
    "\n",
    "def get_explanation(question):\n",
    "    \"\"\"Get the explanation for the given question\"\"\"\n",
    "    return explanations.get(japanese_dataset.get(question, \"\"), \"\")\n",
    "\n",
    "def check_answer(question, user_answer):\n",
    "    \"\"\"Check if the user's answer matches the correct answer\"\"\"\n",
    "    correct_answer = japanese_dataset.get(question, \"\")\n",
    "    return user_answer.strip().lower() == correct_answer.lower()\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(text, vocab):\n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens:\n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w:\n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)\n",
    "\n",
    "def pred_class(text, vocab, labels):\n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.5\n",
    "    y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "0\n",
    "def get_response(intents_list, intents_json):\n",
    "    if len(intents_list) == 0:\n",
    "        result = \"Sorry, I didn't understand that. Can you please provide more information?\"\n",
    "    elif len(intents_list) > 1:\n",
    "        result = \"I'm not sure which response to provide. Can you please clarify?\"\n",
    "    else:\n",
    "        tag = intents_list[0]\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"] == tag:\n",
    "                result = random.choice(i[\"responses\"])\n",
    "                break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zeta: My name is Vestia Zeta. I will answer your queries about Hiraganas and Katakanas. If you want to exit, type 'Bye!'\n",
      "\n",
      "Please choose an action:\n",
      "\n",
      "1. Japanese practice\n",
      "\n",
      "2. Ask me a question regarding hiragana and katakana\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"\"\n",
    "def send_to_chatGPT(user_suggestion):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_suggestion}],\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    print(content)\n",
    "\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"\\nZeta: My name is Vestia Zeta. I will answer your queries about Hiraganas and Katakanas. If you want to exit, type 'Bye!'\")\n",
    "        print(\"\\nPlease choose an action:\")\n",
    "        print(\"\\n1. Japanese practice\")\n",
    "        print(\"\\n2. Ask me a question regarding hiragana and katakana\")\n",
    "        choice = input(\"Enter your choice: \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            flag = True\n",
    "            while flag:\n",
    "                print(\"\\nZeta: Welcome! I will help you learn some Japanese. Type 'Give me a question' to start or type 'return' to return back to the main menu.\")\n",
    "                user_input = input().strip().lower()\n",
    "\n",
    "                if user_input == 'give me a question':\n",
    "                    question = get_random_question()\n",
    "                    print(f\"\\nZeta: What's the meaning of '{question}'?\")\n",
    "                    user_answer = input().strip()\n",
    "                    print(\"\\nYou:\", user_answer)  # Displaying user input\n",
    "\n",
    "                    if check_answer(question, user_answer):\n",
    "                        print(\"\\nZeta: Correct! Well done!\")\n",
    "                        print(\"\\nZeta: Type 'how is it implemented' if you want to know more about the answer.\")\n",
    "                    else:\n",
    "                        print(\"\\nZeta: Sorry, that's not correct. The answer is:\", japanese_dataset[question])\n",
    "                        print(\"\\nZeta: Type 'suggest me' if you would like me to provide you with some suggestion?\")\n",
    "                        user_suggestion_choice = input().lower()\n",
    "                        if user_suggestion_choice == 'suggest me':\n",
    "                            user_suggestion = \"You're a Japanese teacher and this is your student response: \" + user_answer + \" to the question of \" + question + \", please create a suggestion on where is the mistake on how to improve\"\n",
    "                            send_to_chatGPT(user_suggestion)\n",
    "\n",
    "                elif user_input == 'how is it implemented':\n",
    "                    if question:\n",
    "                        explanation = get_explanation(question)\n",
    "                        if explanation:\n",
    "                            print(\"\\nZeta:\", explanation)\n",
    "                        else:\n",
    "                            print(\"\\nZeta: Sorry, I don't have an explanation for that question yet.\")\n",
    "                    else:\n",
    "                        print(\"\\nZeta: You haven't answered any question yet.\")\n",
    "\n",
    "                elif user_input == 'return':\n",
    "                    flag = False\n",
    "                    print(\"\\nZeta: Returning to main menu!\")\n",
    "\n",
    "                else:\n",
    "                    print(\"\\nZeta: I'm sorry, I didn't understand that. Type 'Give me a question' to start.\")\n",
    "\n",
    "        elif choice == \"2\":\n",
    "            while True:\n",
    "                print(\"\\nZeta: Welcome! I will help you answere some question regarding Katakanas and Hiraganas. Type a question to start or type 'return' to return back to the main menu.\")\n",
    "                message = input(\"\")\n",
    "                if message == \"return\":\n",
    "                    break\n",
    "                print(\"\\nYou:\", message)  # Displaying user input\n",
    "                intents = pred_class(message, words, classes)\n",
    "                result = get_response(intents, data)\n",
    "                print(\"Zeta:\", result)\n",
    "\n",
    "        elif choice.lower() == 'bye':\n",
    "            print(\"Zeta: Goodbye! Have a great day!\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"Zeta: Invalid choice. Please enter '1' or '2'. Or type 'Bye' to exit.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
